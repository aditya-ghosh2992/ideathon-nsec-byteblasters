{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-ghosh2992/ideathon-nsec-byteblasters/blob/main/Summarizer_App_using_Palm_Api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project By team Byteblaser\n",
        "Developer by\n",
        "\n",
        "*   Aditya Ghosh\n",
        "*   Ankan Misra\n",
        "*   Suman Jain\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is a python based program developd in the Ideathon'24 organised by NSEC , Kolkata.\n",
        "\n",
        "Project 2 Name :-Text Summarizer WebApp\n",
        "\n",
        "Used Case :- Its understand Your long Text and give you the answer in short brief.\n"
      ],
      "metadata": {
        "id": "nho9cCX7mMRV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NlgwlGYptL6C"
      },
      "outputs": [],
      "source": [
        "!pip install -q google.generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "oO3zZwhTtnkZ",
        "outputId": "2bba49c9-6af5-4eed-e15b-6e1d87298705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm\n",
        "import os"
      ],
      "metadata": {
        "id": "EXF7hTcRt5bs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key=\"AIzaSyASboLpJbVoVifYDJwOX5-z7Ggtr9OgzPg\")"
      ],
      "metadata": {
        "id": "Ws7iOlhyuHcC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(palm.list_models())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lcimMcGi3YFs",
        "outputId": "fdf29a9e-8bf8-4764-a109-86ea43a4058c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in palm.list_models():\n",
        "  print(f\"model.name: {model.name}\")\n",
        "  print(f\"model.description: {model.description}\")\n",
        "  print(f\"model.supported_generation_methods: {model.supported_generation_methods}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "Q2xujCUXuTiZ",
        "outputId": "a7bfc23b-8deb-4ec8-d763-b65979f51e26"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.name: models/chat-bison-001\n",
            "model.description: A legacy text-only model optimized for chat conversations\n",
            "model.supported_generation_methods: ['generateMessage', 'countMessageTokens']\n",
            "model.name: models/text-bison-001\n",
            "model.description: A legacy model that understands text and generates text as an output\n",
            "model.supported_generation_methods: ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
            "model.name: models/embedding-gecko-001\n",
            "model.description: Obtain a distributed representation of a text.\n",
            "model.supported_generation_methods: ['embedText', 'countTextTokens']\n",
            "model.name: models/gemini-1.0-pro\n",
            "model.description: The best model for scaling across a wide range of tasks\n",
            "model.supported_generation_methods: ['generateContent', 'countTokens']\n",
            "model.name: models/gemini-1.0-pro-001\n",
            "model.description: The best model for scaling across a wide range of tasks. This is a stable model that supports tuning.\n",
            "model.supported_generation_methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
            "model.name: models/gemini-1.0-pro-latest\n",
            "model.description: The best model for scaling across a wide range of tasks. This is the latest model.\n",
            "model.supported_generation_methods: ['generateContent', 'countTokens']\n",
            "model.name: models/gemini-1.0-pro-vision-latest\n",
            "model.description: The best image understanding model to handle a broad range of applications\n",
            "model.supported_generation_methods: ['generateContent', 'countTokens']\n",
            "model.name: models/gemini-1.5-flash-latest\n",
            "model.description: Fast and versatile multimodal model for scaling across diverse tasks\n",
            "model.supported_generation_methods: ['generateContent', 'countTokens']\n",
            "model.name: models/gemini-1.5-pro-latest\n",
            "model.description: Mid-size multimodal model that supports up to 1 million tokens\n",
            "model.supported_generation_methods: ['generateContent', 'countTokens']\n",
            "model.name: models/gemini-pro\n",
            "model.description: The best model for scaling across a wide range of tasks\n",
            "model.supported_generation_methods: ['generateContent', 'countTokens']\n",
            "model.name: models/gemini-pro-vision\n",
            "model.description: The best image understanding model to handle a broad range of applications\n",
            "model.supported_generation_methods: ['generateContent', 'countTokens']\n",
            "model.name: models/embedding-001\n",
            "model.description: Obtain a distributed representation of a text.\n",
            "model.supported_generation_methods: ['embedContent']\n",
            "model.name: models/text-embedding-004\n",
            "model.description: Obtain a distributed representation of a text.\n",
            "model.supported_generation_methods: ['embedContent']\n",
            "model.name: models/aqa\n",
            "model.description: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
            "model.supported_generation_methods: ['generateAnswer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [model for model in palm.list_models() if 'generateText' in model.supported_generation_methods]\n",
        "models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "8mtfXiSYvALJ",
        "outputId": "5bb6f85f-9afa-475e-ba55-91b2aea3bee1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_bison = models[0]"
      ],
      "metadata": {
        "id": "q7lXUuKjvNda"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Generate_summary(text):\n",
        "  prompt = f\"\"\"\n",
        "  Your task is to act as a Text Summarizer.\n",
        "  I'll Give you text.\n",
        "  Your task is to create summary for the text in 100 words.\n",
        "  text is shared below, delimited with triple backticks.\n",
        "\n",
        "  ```\n",
        "  {text}\n",
        "  ```\n",
        "  \"\"\"\n",
        "  response = palm.generate_text(\n",
        "      model=model_bison,\n",
        "      prompt=prompt,\n",
        "      temperature=0\n",
        "  )\n",
        "  return response.result"
      ],
      "metadata": {
        "id": "1qrQwd7IvS8c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "My Name is Aakash Singh. Graduated from IIT Bombay. My Home Town is Siwan\n",
        "Distric of Bihar.\n",
        "I've interest in Data Science and Artificial intelligence field.\n",
        "I've been working as an Associate Data Scientist at Hdfc Life.\n",
        "I'm also uploading project related to GenAI on my Youtube Chaneel.\n",
        "My Youtube channel is IITian Aakash Singh[IITB]\n",
        "\"\"\"\n",
        "print(Generate_summary(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "_PoblSqSwkHp",
        "outputId": "9c9e6590-c017-4ae2-e227-8d1ec1ddd496"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aakash Singh is an Associate Data Scientist at HDFC Life. He graduated from IIT Bombay and has an interest in Data Science and Artificial Intelligence. He uploads projects related to GenAI on his YouTube channel, IITian Aakash Singh[IITB].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**gradio APP**"
      ],
      "metadata": {
        "id": "vEa1MrQQxi5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "ycOf8f5uy29r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio\n",
        "interface = gradio.Interface(fn = Generate_summary,\n",
        "                             inputs=[gradio.Textbox(label=\"Enter Text here...\")],\n",
        "                             outputs = [gradio.Textbox(label=\"Output Summary here... \")],\n",
        "                             title=\"Text Summarizer app -- Developed by team ByteBlasters for Ideathon'24 at NSEC\",\n",
        "                             description=\"This Projects Analyse and make a short version of the text. This Project uses Google Colab , PLAM API and Gradio \",\n",
        "                             allow_flagging=\"never\")\n",
        "interface.launch(share=True,debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "-kHqbgfoxQXC",
        "outputId": "a7d9736d-1f6f-4fde-a18d-3f92f2d3277a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ee369da0654607807b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ee369da0654607807b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interface.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72RVGTSrywtb",
        "outputId": "0766b0a1-7339-4900-c568-59e4782aa6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "I'd be happy to explain a project I worked on that involved image classification. The project aimed to build a document classification model using a pre-trained neural network architecture called DenseNet121. The goal was to accurately classify various types of documents such as Aadhar cards, PAN cards, passports, and more.\"\n",
        "Step 1: Importing Libraries\n",
        "The project began by importing the necessary libraries, including NumPy and TensorFlow. TensorFlow is a powerful library for building and training neural networks, while NumPy provides tools for numerical computations. I also imported components from Keras, a high-level neural network API, for using the DenseNet121 pre-trained model\n",
        "Step 2: Setting Constants\n",
        "After importing the libraries, I set up important constants that define aspects of the project. These constants included the data directory path, image dimensions, batch size, number of classes, and the number of epochs for both fine-tuning and the total training process.\n",
        "Step 3: Data Preprocessing and Augmentation\n",
        "Data preprocessing and augmentation are crucial for improving the model's performance. I used an ImageDataGenerator to apply various transformations to the images, such as rotation, shifting, shearing, and more. This helps increase the model's ability to generalize. I also divided the data into training and validation subsets to ensure I could evaluate the model's performance effectively.\"\n",
        "Step 4: Mapping Class Indices to Names\n",
        "In order to keep track of the classes and their respective labels, I mapped class indices to class names using a dictionary. This allowed us to associate human-readable names with the numerical labels and also counted the number of images per class for both the training and validation sets.\n",
        "Step 5: Loading Pre-trained Model\n",
        "To save time and leverage existing knowledge, I used a pre-trained model called DenseNet121. This model has already learned useful features from a large dataset, which I can then fine-tune to our specific classification task. I excluded the fully connected layers at the top of the model to customize it for our own task.\n",
        "Step 6: Fine-tuning by Unfreezing Layers\n",
        "For fine-tuning, I unfroze the last 20 layers of the base model. This allowed us to adapt those layers to the specific characteristics of our document classification problem.\n",
        "Step 7: Adding Custom Layers and Compiling Model\n",
        "I then constructed a new neural network by adding custom layers on top of the pre-trained base model. This included a Global Average Pooling layer to reduce dimensions, a dense layer for classification, and a dropout layer to prevent overfitting. After setting up the architecture, I compiled the model with a low learning rate optimizer to fine-tune it for our task\n",
        "Step 8: Model Training\n",
        "With the model architecture and compilation ready, I moved on to training the model. I used the fit method to train the model on the training data and validated its performance using the validation data. I trained for the specified number of epochs.\n",
        "Step 9: Model Evaluation\n",
        "After training, I evaluated the model's performance on the validation data. I generated predictions for the validation dataset and compared them to the true labels. This allowed us to create a classification report with metrics such as precision, recall, and F1-score. I also calculated a confusion matrix to understand how well the model was classifying different types of documents\n",
        "Step 10: Model Saving\n",
        "Finally, after achieving a satisfactory level of performance, I saved the trained model to a file. This allowed us to reuse the model later without having to retrain it from scratch.\n",
        "In conclusion, this project showcased how to leverage pre-trained neural network architectures, perform fine-tuning, and effectively build and train an image classification model. It's a great example of how machine learning can automate tasks like document sorting or verification, saving time and reducing errors\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Myut0iytz1Ii"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
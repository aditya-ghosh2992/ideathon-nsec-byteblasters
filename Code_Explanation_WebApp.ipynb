{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-ghosh2992/ideathon-nsec-byteblasters/blob/main/Code_Explanation_WebApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project By team Byteblaser\n",
        "Developder by\n",
        "\n",
        "*   Aditya Ghosh\n",
        "*   Ankan Misra\n",
        "*   Suman Jain\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is a python based program developd in the Ideathon'24 organised by NSEC , Kolkata.\n",
        "\n",
        "Project 1 Name :- Code Explanation\n",
        "\n",
        "Used Case :- Its simplify any code to a laymans Language.\n"
      ],
      "metadata": {
        "id": "nho9cCX7mMRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-R3rBEO0mRx9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4ILymDJuoMe"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm\n",
        "import os"
      ],
      "metadata": {
        "id": "76mtAi8DvPTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key='AIzaSyASboLpJbVoVifYDJwOX5-z7Ggtr9OgzPg')"
      ],
      "metadata": {
        "id": "ZrLD58wovyvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(palm.list_models())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WfMIig2HO8iV",
        "outputId": "421629e0-e396-4123-e304-4173e9e7a823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
        "model = models[0].name\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v_Ng9LJr0uUs",
        "outputId": "20ec440f-3cac-4225-b9e2-6e54d0c3d3f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/text-bison-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF1caY0s6eAr",
        "outputId": "1ed48885-5c3a-4d7a-f9f7-77330b249d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Chatbot**"
      ],
      "metadata": {
        "id": "IRNBhUFz6m3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Your Input Text\n",
        "prompt1 = \"Why is the Sky blue?\"\n",
        "Prompt2 = \"Why Generative AI growing too much?\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model = model,\n",
        "    prompt = prompt1,\n",
        "    temperature = 0,\n",
        "    # temperature =0 >> more deterministic results // temperature = 1 >> more randomness\n",
        "    max_output_tokens = 100\n",
        ")\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "8pmhnMMA6p62",
        "outputId": "2b36b973-2863-4c96-9db0-9257ddeb6311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky is blue because of a phenomenon called Rayleigh scattering. This is the scattering of light by particles that are smaller than the wavelength of light. In the case of the sky, the particles are molecules of nitrogen and oxygen.\n",
            "\n",
            "When sunlight hits these molecules, the blue light is scattered more than the other colors because it has a shorter wavelength. This is because the blue light waves are more likely to interact with the molecules of nitrogen and oxygen. The other colors, such as red and yellow,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Your Input Text\n",
        "prompt1 = \"Why is the Sky blue?\"\n",
        "prompt2 = \"Why Generative AI growing too much? Provide answers in bullain format in clear and concise maner\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model = model,\n",
        "    prompt = prompt2,\n",
        "    temperature = 0,\n",
        "    # temperature =0 >> more deterministic results // temperature = 1 >> more randomness\n",
        "    max_output_tokens = 100\n",
        ")\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "k86thLvS7qdt",
        "outputId": "4f26564e-b100-48b6-b89e-54e5a3bc03d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Why Generative AI is growing so much:**\n",
            "\n",
            "* **The amount of data available is growing exponentially.** This means that there is more and more data for generative AI models to learn from.\n",
            "* **The computing power available is also growing exponentially.** This means that generative AI models can be trained on larger and larger datasets.\n",
            "* **The algorithms for generative AI are becoming more sophisticated.** This means that generative AI models are able to learn more complex relationships\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt):\n",
        "  completion = palm.generate_text(\n",
        "      model=model,\n",
        "      prompt=prompt,\n",
        "      temperature=0,\n",
        "      # The maximum length of the response\n",
        "      max_output_tokens=500\n",
        "  )\n",
        "  response = completion.result\n",
        "  return response"
      ],
      "metadata": {
        "id": "WyADRKKF-Sig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Prompting**"
      ],
      "metadata": {
        "id": "XEab6Q9_9MVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_code = f\"\"\"\n",
        "x = [1,2,3,4,5]\n",
        "y = [i*i for i in x if i%2==0]\n",
        "print(y)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FOSv-jqL9Prh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "your Task is to act as a Python Code Explainer.\n",
        "I'll give you Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Also, compute the final output of the code.\n",
        "Code Snippet is shared below, delimited with triple backticks.\n",
        "```\n",
        "{input_code}\n",
        "```\n",
        "\n",
        "\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuE4xLPj9jQW",
        "outputId": "43a7aa41-2c4c-4e36-97ca-59c42e53243a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "your Task is to act as a Python Code Explainer.\n",
            "I'll give you Code Snippet.\n",
            "Your Job is to explain the Code Snippet step-by-step.\n",
            "Also, compute the final output of the code.\n",
            "Code Snippet is shared below, delimited with triple backticks.\n",
            "```\n",
            "\n",
            "x = [1,2,3,4,5]\n",
            "y = [i*i for i in x if i%2==0]\n",
            "print(y)\n",
            "\n",
            "```\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "4XjOGzqv-7NJ",
        "outputId": "f7439989-a254-4f35-d34d-2ef3accc7c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "# Create a list x with numbers from 1 to 5\n",
            "x = [1,2,3,4,5]\n",
            "\n",
            "# Create a new list y that contains the squares of the even numbers in x\n",
            "y = [i*i for i in x if i%2==0]\n",
            "\n",
            "# Print the list y\n",
            "print(y)\n",
            "\n",
            "```\n",
            "\n",
            "Output:\n",
            "```\n",
            "[4, 16]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEwKUkI5ETeH",
        "outputId": "763a7539-2bdd-4a05-bb44-ffb3c97f9ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LowHighLow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_code = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R5q8x-uhD0MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to act as a Python Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your job is to explain the Code Snippet Step-by-Step.\n",
        "Also, compute the final output of the code.\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "\n",
        "```\n",
        "{input_code}\n",
        "```\n",
        "\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA4cwQv-EgQN",
        "outputId": "ee78e099-d3a8-4227-fd20-98d665b02018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Your task is to act as a Python Code Explainer.\n",
            "I'll give you a Code Snippet.\n",
            "Your job is to explain the Code Snippet Step-by-Step.\n",
            "Also, compute the final output of the code.\n",
            "Code Snippet is shared below, delimited with triple backticks:\n",
            "\n",
            "```\n",
            "\n",
            "def my_func(x):\n",
            "  if x>5:\n",
            "    return \"High\"\n",
            "  else:\n",
            "    return \"Low\"\n",
            "\n",
            "result = my_func(4) + my_func(6) + my_func(4)\n",
            "print(result)\n",
            "\n",
            "\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "nldXvoQ3FNWd",
        "outputId": "7412728e-1f66-4cec-bfd8-a210455e4a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-by-Step:\n",
            "1. The function `my_func` takes an integer as input and returns `\"High\"` if the input is greater than 5, otherwise it returns `\"Low\"`.\n",
            "2. The variable `result` is assigned the value of `my_func(4)` + `my_func(6)` + `my_func(4)`.\n",
            "3. The value of `result` is printed to the console, which is `HighLowLow`.\n",
            "\n",
            "Final output:\n",
            "```\n",
            "HighLowLow\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Effective Prompting**"
      ],
      "metadata": {
        "id": "sqQGl7qQFgi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "  global x\n",
        "  x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lL4f0x46FkD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_code = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = my_func(4) + my_func(6) + my_func(4)\n",
        "print(result)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xudiEApxG0Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "  Your Task is to act as Python Code Explainer.\n",
        "  I'll give you a Code Snippet.\n",
        "  Your Job is to explain the Code Snippet step-by-step.\n",
        "  Break down the code into as many steps as possible.\n",
        "  Share intermediate checkpoints & steps along with results.\n",
        "  Few good examples of python code output between #### seperator:\n",
        "  ####\n",
        "  {python_code_example}\n",
        "  ####\n",
        "  Code Snippet is shared below, delimited with triple backticks:\n",
        "  ```\n",
        "  {input_code}\n",
        "  ```\n",
        "  \"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYd5UilqG-rH",
        "outputId": "80f5cec3-ac39-4da1-d5f1-a8ef618d3490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Your Task is to act as Python Code Explainer.\n",
            "  I'll give you a Code Snippet.\n",
            "  Your Job is to explain the Code Snippet step-by-step.\n",
            "  Break down the code into as many steps as possible.\n",
            "  Share intermediate checkpoints & steps along with results.\n",
            "  Few good examples of python code output between #### seperator:\n",
            "  ####\n",
            "  \n",
            "----------------------------\n",
            "Example 1: Code Snippet\n",
            "x = 10\n",
            "def foo():\n",
            "  global x\n",
            "  x = 5\n",
            "\n",
            "foo()\n",
            "print(x)\n",
            "Correct output: 5\n",
            "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
            "So, print(x) outside the function print the modfied value is 5.\n",
            "-----------------------------\n",
            "\n",
            "Example 2: Code Snippet\n",
            "def modify_list(input_list):\n",
            "  input_list.append(4)\n",
            "  input_list = [1,2,3]\n",
            "my_list = [0]\n",
            "modify_list(my_list)\n",
            "print(my_list)\n",
            "Correct output: [0, 4]\n",
            "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
            "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
            "So, print(my_list) outputs [0, 4].\n",
            "------------------------------\n",
            "\n",
            "  ####\n",
            "  Code Snippet is shared below, delimited with triple backticks:\n",
            "  ```\n",
            "  \n",
            "def my_func(x):\n",
            "  if x>5:\n",
            "    return \"High\"\n",
            "  else:\n",
            "    return \"Low\"\n",
            "\n",
            "result = my_func(4) + my_func(6) + my_func(4)\n",
            "print(result)\n",
            "\n",
            "\n",
            "  ```\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "SV-DwAGPHKG5",
        "outputId": "11f6eeae-ea4e-4759-db07-fc5fb8b75067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####\n",
            "  Code Explaination:\n",
            "Step 1: Define a function my_func that takes an integer as input and returns \"High\" if the input is greater than 5, else returns \"Low\".\n",
            "Step 2: Define a variable result and assign it the value of my_func(4) + my_func(6) + my_func(4).\n",
            "Step 3: Print the value of result.\n",
            "\n",
            "Intermediate checkpoints:\n",
            "Step 1: my_func(4) returns \"Low\".\n",
            "Step 2: my_func(6) returns \"High\".\n",
            "Step 3: my_func(4) returns \"Low\".\n",
            "Step 4: result is assigned the value of \"Low\" + \"High\" + \"Low\".\n",
            "Step 5: result is printed and the output is \"LowHighLow\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradia App**"
      ],
      "metadata": {
        "id": "D8r-Pt9U0_uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "N-sjjQs-1Q9S",
        "outputId": "75be6f40-d948-488f-a505-da4c35dbbd88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define completion function\n",
        "def get_completion(code_snippet):\n",
        "\n",
        "  python_code_example = f\"\"\"\n",
        "  ----------------------------\n",
        "  Example 1: Code Snippet\n",
        "  x = 10\n",
        "  def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "  foo()\n",
        "  print(x)\n",
        "  Correct output: 5\n",
        "  Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "  So, print(x) outside the function print the modfied value is 5.\n",
        "  -----------------------------\n",
        "\n",
        "  Example 2: Code Snippet\n",
        "  def modify_list(input_list):\n",
        "    input_list.append(4)\n",
        "    input_list = [1,2,3]\n",
        "  my_list = [0]\n",
        "  modify_list(my_list)\n",
        "  print(my_list)\n",
        "  Correct output: [0, 4]\n",
        "  Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "  Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "  So, print(my_list) outputs [0, 4].\n",
        "  ------------------------------\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Your Task is to act as Python Code Explainer.\n",
        "  I'll give you a Code Snippet.\n",
        "  Your Job is to explain the Code Snippet step-by-step.\n",
        "  Break down the code into as many steps as possible.\n",
        "  Share intermediate checkpoints & steps along with results.\n",
        "  Few good examples of python code output between #### seperator:\n",
        "  ####\n",
        "  {python_code_example}\n",
        "  ####\n",
        "  Code Snippet is shared below, delimited with triple backticks:\n",
        "  ```\n",
        "  {code_snippet}\n",
        "  ```\n",
        "  \"\"\"\n",
        "  completion = palm.generate_text(\n",
        "    model = model,\n",
        "    prompt = prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length if the response\n",
        "    max_output_tokens=500,\n",
        "  )\n",
        "  response = completion.result\n",
        "  return response"
      ],
      "metadata": {
        "id": "lxQnBKh21rp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradio.Interface(get_completion, \"text\", \"text\").launch(share=False)"
      ],
      "metadata": {
        "id": "PtonjN6XAny1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define app UI\n",
        "import gradio\n",
        "\n",
        "iface = gradio.Interface(fn=get_completion, inputs=[gradio.Textbox(label=\"Insert Code Snippet\")],\n",
        "                     outputs=[gradio.Textbox(label=\"Explanation Here\")],\n",
        "                     title=\"Code Explainer -- Developed by team ByteBlasters for Ideathon'24 at NSEC\",\n",
        "                     description=\"This Projects Analyse and explain code easy and Best work with Python  This Project uses Google Colab , PLAM API and Gradio \",\n",
        "                     allow_flagging=\"never\",)\n",
        "\n",
        "\n",
        "iface.launch(share=True,debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j_YDHUgZ485y",
        "outputId": "20ee081f-13f2-43d2-8553-2d8c920dc028"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f4505eaee51d24160a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://f4505eaee51d24160a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 537, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 461, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "TimeoutError: timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 486, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 845, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 470, in increment\n",
            "    raise reraise(type(error), error, _stacktrace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/util.py\", line 39, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 791, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 539, in _make_request\n",
            "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 371, in _raise_timeout\n",
            "    raise ReadTimeoutError(\n",
            "urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=33909): Read timed out. (read timeout=60.0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 528, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1908, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1485, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-22-f438789f5b9d>\", line 49, in get_completion\n",
            "    completion = palm.generate_text(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 202, in generate_text\n",
            "    return _generate_response(client=client, request=request, request_options=request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 240, in _generate_response\n",
            "    response = client.generate_text(request, **request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/text_service/client.py\", line 870, in generate_text\n",
            "    response = rpc(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 113, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 349, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 191, in retry_target\n",
            "    return target()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\", line 120, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 72, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/text_service/transports/rest.py\", line 667, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/auth/transport/requests.py\", line 541, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 532, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=33909): Read timed out. (read timeout=60.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 537, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 461, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "TimeoutError: timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 486, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 845, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 470, in increment\n",
            "    raise reraise(type(error), error, _stacktrace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/util.py\", line 39, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 791, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 539, in _make_request\n",
            "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 371, in _raise_timeout\n",
            "    raise ReadTimeoutError(\n",
            "urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=33909): Read timed out. (read timeout=60.0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 528, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1908, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1485, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-22-f438789f5b9d>\", line 49, in get_completion\n",
            "    completion = palm.generate_text(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 202, in generate_text\n",
            "    return _generate_response(client=client, request=request, request_options=request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 240, in _generate_response\n",
            "    response = client.generate_text(request, **request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/text_service/client.py\", line 870, in generate_text\n",
            "    response = rpc(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 113, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 349, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 191, in retry_target\n",
            "    return target()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\", line 120, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 72, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/text_service/transports/rest.py\", line 667, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/auth/transport/requests.py\", line 541, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 532, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=33909): Read timed out. (read timeout=60.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 537, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 461, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "TimeoutError: timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 486, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 845, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 470, in increment\n",
            "    raise reraise(type(error), error, _stacktrace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/util.py\", line 39, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 791, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 539, in _make_request\n",
            "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 371, in _raise_timeout\n",
            "    raise ReadTimeoutError(\n",
            "urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=33909): Read timed out. (read timeout=60.0)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 528, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1908, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1485, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-22-f438789f5b9d>\", line 49, in get_completion\n",
            "    completion = palm.generate_text(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 202, in generate_text\n",
            "    return _generate_response(client=client, request=request, request_options=request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/generativeai/text.py\", line 240, in _generate_response\n",
            "    response = client.generate_text(request, **request_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/text_service/client.py\", line 870, in generate_text\n",
            "    response = rpc(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\", line 113, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 349, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\", line 191, in retry_target\n",
            "    return target()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\", line 120, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\", line 72, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/text_service/transports/rest.py\", line 667, in __call__\n",
            "    response = getattr(self._session, method)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/auth/transport/requests.py\", line 541, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 532, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=33909): Read timed out. (read timeout=60.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "id": "_9CmH0Cf6lKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yeq3GbZd7xvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}